---
title: "Engineering Document"
author: "Michael Lanier"
date: "October 29, 2017"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r, echo=F}
data_master=read.csv(file="Toms_data_consl.csv",header=TRUE)

tot_reg=max(data_master$Fit_num)
library(plotly)
library(gvlma)
library(MASS)
library(car)

```

##General Notes and Summary

* Linear regressions are performed.
* Linked Function failure indicates non linearity. In this case a quadratic regression is fit.
* Quadratic fits have adjusted X values such that Xadj=X-mean(X). This is to prevent a regression issue around colinear predictors. Equations are given in terms of Xadj because the author is lazy.
* Regression 23, 38, 46, 64 , 77 are insignificent. Ie There is no evidence for a relationship between X and Y.
* Regression 37, 69 have a insignifient X but a significent X^2. It is not logically possible for this to occur. Both are included in this case.
* In the event that the predictor is non significent (asterisks next to predictors indicate significence), no relationship may exist or a simple average is the best predictor.
* Loess is locally weighted scatter plot smoothing. It is essentially a smoother moving average. It is included to highlight the fit.
* No regression is done if the response is constant. The equation would just be y=constant. Similarly no constant predictor is included because it's coefficent is 0.
```{r , echo=FALSE,warning =F,messages=F,errors=F}

for( i in 1:tot_reg)
  
  
{


  sub=subset(data_master,data_master$Fit_num==i)

  if(var(sub$Y)>0)
  {
  fit=lm(Y~X1,data=sub)
  print(paste("Fit for Regression",i,sep=" "))
  titl1=paste("Regression Equation: y=",toString(round(fit$coefficients[[2]],digits=3)),"x+",toString(round(fit$coefficients[[1]],digits=3)),sep="")
    if(gvlma(fit)$GlobalTest$DirectionalStat3[[3]]==1)
      {
      #if(min(sub$Y)<0)
      #{
      #  offset=1+-1*min(sub$Y)
      #  sub$Y= sub$Y+offset
     # }else{offset=0}
     # df=boxcox(fit, lambda = seq(-2, 2, 1/10),data=sub)
     # L=df$x[which.max(df$y)]
      #if(L<0)
      #{sub=subset(sub,sub$Y!=0)}
      #depvar.transformed <- yjPower(sub$Y, L)
      sub$X1=sub$X1-mean(sub$X1)
      fit=lm(Y~X1+I(X1^2),data=sub) 
      #titl2= paste("Regression Equation: (y-",toString(offset),")^",round(L,digits=3),"=",toString(round(fit$coefficients[[2]],digits=3)),"x",L,toString(round(fit$coefficients[[1]],digits=3)),sep="")
      titl2= paste("Regression Equation: y=",toString(round(fit$coefficients[[3]],digits=3)),"xadj^2+",toString(round(fit$coefficients[[2]],digits=3)),"xadj+",toString(round(fit$coefficients[[1]],digits=3)),sep="")
      
      summary(gvlma(fit) )
      g=ggplot(data = sub, aes(x = X1, y = Y)) +
      stat_smooth(method = 'glm', aes(colour = 'lm'),formula=y ~ poly(x, 2)) +
      stat_smooth(method = 'loess', aes(colour = 'loess')) +
      geom_point(size=2, shape=23)+
      labs(title = titl2 )
    print(g) 
        }else{
  
    summary(fit)
      print(summary(fit))
      g=ggplot(data = sub, aes(x = X1, y = Y)) +
      stat_smooth(method = 'lm', aes(colour = 'linear'), se = T) +
      stat_smooth(method = 'loess', aes(colour = 'loess')) +
      geom_point(size=2, shape=23)+
      labs(title = titl1 )
    print(g)}

  
  
  }else{
    
    
    print(paste("Variance in response is zero for fit",i,sep=" "))
  }
  
}

```


